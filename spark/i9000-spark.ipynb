{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:2.5em;color:purple; font-style:bold\"><br/>\n",
    "I9000平台Spark使用基础\n",
    "</p>\n",
    "\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "I9000云平台鸟瞰（复习）\n",
    "</p>\n",
    "\n",
    "- 应用发布与管理平台\n",
    "- 分布式存储与计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "大数据存储与计算最常用的两个工具\n",
    "</p>\n",
    "\n",
    "- HDFS\n",
    "  - Hadoop分布式文件系统\n",
    "- Spark\n",
    "  - 分布式计算引擎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "HDFS – Hadoop分布式文件系统\n",
    "</p>\n",
    "\n",
    "- 文件系统\n",
    "  - 树形结构组织的目录和文件\n",
    "  - Linux命令行使用ls、cd等命令切换和查看\n",
    "  - Windows使用我的电脑查看\n",
    "  - 在编程语言例如Python中，通过调用函数访问\n",
    "- HDFS分布式文件系统，使用者角度\n",
    "  - 与普通文件系统类似\n",
    "  - 主要的区别是能够存放大文件，比如一个文件几百GB或几个T\n",
    "- HDFS作为数据存储层，属于I9000系统的组成部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "HDFS为什么能够存储大文件（架构概览）\n",
    "</p>\n",
    "\n",
    "<img src=\"i9000-spark-image/hdfs.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "在I9000中使用HDFS\n",
    "</p>\n",
    "\n",
    "- HDFS是I9000的数据层\n",
    "- 通过I9000的Web界面上传数据\n",
    "  - 存放在HDFS中\n",
    "- 使用这些文件\n",
    "  - 通过文件在HDFS上的URI来访问文件"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "练习 – 上传文件到HDFS\n",
    "</p>\n",
    "\n",
    "- 本地文件路径（XXX替换为云桌面用户名）\n",
    "\n",
    "> /home/XXX/i9000-training/spark/spark-readme.md\n",
    "\n",
    "- 请保存上传文件的URI，下面练习会用到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "Spark – 分布式计算引擎\n",
    "</p>\n",
    "\n",
    "- 统一的平台\n",
    "  - 数据加载、SQL分析、机器学习、流式计算\n",
    "  - 一致性的编程接口\n",
    "  - 同时支持交互式分析和应用程序开发\n",
    "- Spark只是计算引擎，可以采用多种数据引擎\n",
    "  - HDFS\n",
    "  - Sql数据库\n",
    "  - NoSql数据库\n",
    "  - 消息队列，例如Apache Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "Spark应用程序组成\n",
    "</p>\n",
    "\n",
    "<img src=\"i9000-spark-image/spark-overview.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "Spark多语言支持\n",
    "</p>\n",
    "\n",
    "- Spark核心是Scala语言编写\n",
    "  - 运行在JVM上\n",
    "- Spark支持的用户语言\n",
    "  - Scala\n",
    "  - Java\n",
    "  - Python\n",
    "  - R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "使用Python时的进程关系\n",
    "</p>\n",
    "\n",
    "<img src=\"i9000-spark-image/python-process.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "Spark应用编程接口（API）\n",
    "</p>\n",
    "\n",
    "- 低层无结构的RDD\n",
    "- 高层有结构的DataFrame\n",
    "- 高层DataFrame建筑在低层RDD之上"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "Spark编程入口\n",
    "</p>\n",
    "\n",
    "- 2.x版本\n",
    "  - SparkSession\n",
    "- 1.x版本\n",
    "  - SparkContext\n",
    "- 今天的例子使用1.x版本"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "I9000中Spark交互式环境\n",
    "</p>\n",
    "\n",
    "- Python Spark Shell\n",
    "- Jupyter Notebook\n",
    "  - 讲稿正在用的环境"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "Python Spark Shell\n",
    "</p>\n",
    "\n",
    "- 使用Python语言的Spark交互环境\n",
    "- 在I9000中启动\n",
    "  - Jupyter terminal\n",
    "- 自动创建SparkContext对象\n",
    "  - pyspark中，绑定到sc变量上\n",
    "- SparkContext位于驱动进程（driver）中\n",
    "  - 是用户代码与Spark集群沟通的桥梁\n",
    "- 每个Spark应用程序对应一个SparkContext对象"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "低层编程接口 - RDD\n",
    "</p>\n",
    "\n",
    "- Resilient Distributed Dataset\n",
    "  - 弹性分布式数据集\n",
    "- 想象为一个大的列表，类似python的list\n",
    "  - 不同的是，元素存放在集群的多个节点中\n",
    "- RDD是不可变的\n",
    "  - 修改RDD，不会改变旧的，而是创建新的RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "RDD的创建\n",
    "</p>\n",
    "\n",
    "- 从现有序列创建\n",
    "  - SparkContext对象的parallelize()方法\n",
    "- 从外部存储系统创建，例如HDFS\n",
    "  - 例如文本文件：SparkContext对象的textFile()方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入python spark库\n",
    "import pyspark\n",
    "\n",
    "# 创建SparkContext对象\n",
    "sc = pyspark.SparkContext()\n",
    "\n",
    "# 将本地列表分发到集群，创建RDD\n",
    "rdd_1 = sc.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "# 查看RDD的前三个元素\n",
    "print(rdd_1.take(3))\n",
    "\n",
    "# 收集集群中RDD的所有元素到本地\n",
    "print(rdd_1.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "RDD的两类操作\n",
    "</p>\n",
    "\n",
    "- 转换操作（transformation）\n",
    "  - 输入一个RDD，输出一个新的RDD\n",
    "- action操作\n",
    "  - 输入一个RDD，输出一个值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "RDD的转换操作示例 - map\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_rdd = rdd_1.map(lambda x: x * 2)\n",
    "new_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "RDD的action操作示例 - reduce\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_1.reduce(lambda result, x: result + x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "练习中用到的两个操作\n",
    "</p>\n",
    "\n",
    "- flatMap\n",
    "- reduceByKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_rdd = rdd_1.flatMap(lambda x: [x, x])\n",
    "new_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_2 = sc.parallelize([('a', 1), ('a', 2), ('b', 5), ('b', 5), ('b', 2)])\n",
    "new_rdd = rdd_2.reduceByKey(lambda result, x: result + x)\n",
    "new_rdd.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "练习 - Spark版单词计数\n",
    "</p>\n",
    "\n",
    "1. 打开I9000的jupyter terminal\n",
    "2. 运行pyspark\n",
    "3. 输入下面代码运行，一次一行\n",
    "\n",
    "** hdfs://XXX 替换为刚才上传文件在HDFS上的URI **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines = sc.textFile('hdfs://XXX')\n",
    "f = lambda line: [(str(word).lower(), 1) for word in line.split()]\n",
    "words = lines.flatMap(f)\n",
    "word_count_rdd = words.reduceByKey(lambda total, count: total + count)\n",
    "print(word_count_rdd.take(3))\n",
    "word_to_count = dict(word_count_rdd.collect())\n",
    "print(word_to_count['spark'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "美国加州房屋数据\n",
    "</p>\n",
    "\n",
    "- block group（区块组）\n",
    "  - 一行是一个block group\n",
    "  - 一个block group的人口数量平均在一两千\n",
    "- 数据共有9列\n",
    "  - longitude：经度\n",
    "  - latitude：纬度\n",
    "  - housingMedianAge：区块组的居民的年龄中位数\n",
    "  - totalRooms：区块组的房屋内的房间总数\n",
    "  - totalBedrooms：区块组的房屋内的卧室总数\n",
    "  - population：区块组的居民的数量\n",
    "  - households：区块组的房屋数量\n",
    "  - medianIncome：区块组的居民收入中位数\n",
    "  - medianHouseValue：区块组的房屋价值中位数\n",
    "\n",
    "> ** medianHouseValue是输出（因变量） ** <br/>\n",
    "> ** 其余变量是输入（自变量） **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "练习 - 加载并查看房屋数据\n",
    "</p>\n",
    "\n",
    "1. 上传房屋数据到HDFS\n",
    "  - /home/XXX/i9000-training/spark/house/cal_housing.data\n",
    "  - /home/xxx/i9000-training/spark/house/cal_housing.domain\n",
    "2. 在spark shell中运行下面的程序语句，一次一条"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 从HDFS文件中创建RDD\n",
    "lines = sc.textFile('XXX')\n",
    "header = sc.textFile('YYY')\n",
    "\n",
    "# 查看列信息\n",
    "print(header. collect())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看前三条数据\n",
    "print(lines.take(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "将RDD转换为DataFrame\n",
    "</p>\n",
    "\n",
    "DataFrame是结构化的类型，类似于数据库的表或excel表格"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 导入相关模块\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# 将原始文本行转为浮点数列表\n",
    "f = lambda line: [float(sp) for sp in line.split(', ')]\n",
    "rows = lines.map(f)\n",
    "\n",
    "# 定义schema（DataFrame的结构描述）\n",
    "names = 'longitude,latitude,housingMedianAge,totalRooms,totalBedrooms,population,households,medianIncome,medianHouseValue'\n",
    "fields = [StructField(name, FloatType()) for name in names.split(',')]\n",
    "schema = StructType(fields)\n",
    "\n",
    "# 创建DataFrame\n",
    "sqlContext = SQLContext(sc)\n",
    "df = sqlContext.createDataFrame(rows, schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "DataFrame操作示例\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 显示数据摘要\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 显示DataFrame的前5条\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 显示DataFrame的schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 显示人口数量和卧室数量两列内容\n",
    "df.select('population', 'totalBedrooms').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 选择年龄中位数超过40的区块组\n",
    "df.filter(df['housingMedianAge'] > 40).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 按年龄中位数分组统计区块组的数量\n",
    "df.groupBy(\"housingMedianAge\").count().show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "DataFrame数据预处理示例 - 调整目标列的单位\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看调整前的目标列\n",
    "df.select('medianHouseValue').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 调整为以10万为单位\n",
    "from pyspark.sql.functions import *\n",
    "df = df.withColumn('medianHouseValue', col('medianHouseValue')/100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 查看调整后的目标列\n",
    "df.select('medianHouseValue').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-family: Arial; font-size:1.5em;color:purple; font-style:bold\">\n",
    "练习 - DataFrame练习\n",
    "</p>\n",
    "\n",
    "- 程序语句来源\n",
    "  - /home/XXX/i9000-training/spark/house/house.py\n",
    "- 在Spark Shell中运行下面两部分程序语句\n",
    "  - 房屋数据练习 - 创建DataFrame\n",
    "  - 房屋数据练习 - DataFrame相关操作"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
